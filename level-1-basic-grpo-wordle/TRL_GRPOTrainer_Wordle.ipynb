{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Fine-Tuning with GRPO on WORDLE\n",
    "\n",
    "This is the level 0 notebook of a series of notebook showcasing the capabilities of Reinforcement Fine-Tuning (RFT). Through carefully designed reward function and SOTA optimization algorithm GRPO, this notebook demonstrates how an RFT tuned model has better reasoning capabilities and has emerging agentic behaviors, e.g. context-aware multi-step reasonings and self-corrections.\n",
    "\n",
    "We will be using a WORDLE utils which defines the basic structure of the game and HuggingFace TRL's GRPO Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt --no-cache-dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.11 (main, Jun 20 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "Torch: 2.7.0+cu126 CUDA available: True\n",
      "Device: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__, \"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566f426d81984ae688b55bde98c19696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib, utils\n",
    "\n",
    "# keep existing objects\n",
    "tok = utils._get_tokenizer()\n",
    "mdl = utils._get_model()\n",
    "\n",
    "# reload edited code\n",
    "importlib.reload(utils)\n",
    "\n",
    "# reattach cached objects to the reloaded module\n",
    "utils._tokenizer = tok\n",
    "utils._model = mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Wordle\n",
    "\n",
    "The Utils file implements the necessary helper functions for Wordle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret word set (for reward function): BRICK\n"
     ]
    }
   ],
   "source": [
    "# Imports and helpers\n",
    "from utils import (\n",
    "    GuessWithFeedback,\n",
    "    get_messages,\n",
    "    render_prompt,\n",
    "    generate,\n",
    "    generate_stream,\n",
    "    next_turn,\n",
    "    get_feedback,\n",
    "    train_wordle_grpo,\n",
    ")\n",
    "\n",
    "secret_word = \"BRICK\"  # change as desired\n",
    "past_guesses = []\n",
    "\n",
    "print(\"Secret word set (for reward function):\", secret_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of our base model playing a few turns of Wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_guesses = []\n",
    "adapter_id = \"wordle-dlai/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the initial guess, I'll choose a common 5-letter word that covers a variety of letters. A good starting point could be \"CRANE\" since it includes vowels and consonants.</think>\n",
      "\n",
      "<guess> CRANE </guess>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CRANE → Feedback: C(-) R(✓) A(x) N(x) E(x)\n"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the feedback, we know that 'R' is correct and in the right position. The other letters (C, A, N, E) are not part of the secret word. So, we should avoid these letters in our next guesses. Let's try a guess that includes 'R' and avoids C, A, N, E.</think>\n",
      "\n",
      "<guess>DRIVE</guess>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CRANE → Feedback: C(-) R(✓) A(x) N(x) E(x)\n",
      "DRIVE → Feedback: D(x) R(✓) I(✓) V(x) E(x)\n"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the feedback:\n",
      "- Guess 1 (CRANE): R is correct and in the right place; A, N, and E are not in the word.\n",
      "- Guess 2 (DRIVE): R is correct and in the right place; I is correct and in the right place; D and V are not in the word.\n",
      "\n",
      "The word must contain 'R' and 'I'. The letters 'A', 'N', 'E', 'D', and 'V' are excluded. The other letters in the guesses are either wrong or in the wrong positions.\n",
      "\n",
      "Considering these constraints, let's try a word that includes 'R' and 'I' and excludes the other letters.</think>\n",
      "<guess> FIRE </guess>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CRANE → Feedback: C(-) R(✓) A(x) N(x) E(x)\n",
      "DRIVE → Feedback: D(x) R(✓) I(✓) V(x) E(x)\n",
      "FIRE → Feedback: F(x) I(-) R(-) E(x)\n"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the previous guesses, we know:\n",
      "- 'R' is in the correct position (2nd).\n",
      "- 'I' and 'E' are not in the word at all.\n",
      "- 'F' is also not in the word.\n",
      "- 'C', 'A', 'N', 'D', 'V' are also not in the word based on their positions or non-inclusion.\n",
      "\n",
      "This leaves us with letters like 'L', 'M', 'T', 'S', 'B', 'G', etc., which haven't been tested yet. Given that 'R' and 'I' are confirmed, let's test another combination ensuring these constraints.</think>\n",
      "\n",
      "<guess>BRIDE</guess>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CRANE → Feedback: C(-) R(✓) A(x) N(x) E(x)\n",
      "DRIVE → Feedback: D(x) R(✓) I(✓) V(x) E(x)\n",
      "FIRE → Feedback: F(x) I(-) R(-) E(x)\n",
      "BRIDE → Feedback: B(✓) R(✓) I(✓) D(x) E(x)\n"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the feedback:\n",
      "1. Guess 1: CRANE (R is correct, in the second position; all other letters are incorrect).\n",
      "2. Guess 2: DRIVE (R is correct, in the second position; I is correct, in the third position; all other letters are incorrect).\n",
      "3. Guess 3: FIRE (I is in the word but not in the right position; R is not in the word; E is correct, in the fifth position).\n",
      "4. Guess 4: BRIDE (B and R are correct, in the first and second positions respectively; I is correct, in the third position; D is not in the word).\n",
      "\n",
      "Combining these, we can deduce:\n",
      "- R is in the second position.\n",
      "- I is in the third position.\n",
      "- E is in the fifth position.\n",
      "- B is in the first position.\n",
      "- We need to determine the fourth letter which is not D (from BRIDE) and should fit with the given constraints.\n",
      "\n",
      "Possible candidates for the fourth letter could be from the remaining alphabet, but let's test a common word that fits these criteria.</think>\n",
      "<guess>BRIST</guess>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CRANE → Feedback: C(-) R(✓) A(x) N(x) E(x)\n",
      "DRIVE → Feedback: D(x) R(✓) I(✓) V(x) E(x)\n",
      "FIRE → Feedback: F(x) I(-) R(-) E(x)\n",
      "BRIDE → Feedback: B(✓) R(✓) I(✓) D(x) E(x)\n",
      "BRIST → Feedback: B(✓) R(✓) I(✓) S(x) T(x)\n"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "- From Guess 1, we know R is in the correct position (2nd).\n",
      "- From Guess 2, we know R is still correct, and I is in the correct position (3rd).\n",
      "- From Guess 3, E is the only consistent letter, so it's in the correct position (5th).\n",
      "- From Guess 4, B and R are confirmed as the 1st and 2nd letters respectively, and I is confirmed as the 3rd letter.\n",
      "- From Guess 5, S is in the correct position (4th).\n",
      "\n",
      "So far, we have B R I S _.\n",
      "\n",
      "Now, let's check for any missing letters. The letters used so far are B, R, I, S, and E. We need to consider the remaining letters that could fit the pattern.\n",
      "\n",
      "The word must contain the letters B, R, I, S, and E, and should not contain any other letters from the previous guesses. Let's try to think of common 5-letter words fitting this pattern.\n",
      "\n",
      "</think>\n",
      "<guess> BRISE </guess>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CRANE → Feedback: C(-) R(✓) A(x) N(x) E(x)\n",
      "DRIVE → Feedback: D(x) R(✓) I(✓) V(x) E(x)\n",
      "FIRE → Feedback: F(x) I(-) R(-) E(x)\n",
      "BRIDE → Feedback: B(✓) R(✓) I(✓) D(x) E(x)\n",
      "BRIST → Feedback: B(✓) R(✓) I(✓) S(x) T(x)\n",
      "BRISE → Feedback: B(✓) R(✓) I(✓) S(x) E(x)\n",
      "❌ better luck next time... ❌\n"
     ]
    }
   ],
   "source": [
    "next_turn(past_guesses, secret_word, adapter_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with TRL GRPO\n",
    "\n",
    "We run a short GRPO session to improve performance on the Wordle-style task. The reward function parses the model's `<guess>...</guess>` and rewards correct-position matches plus an exact-match bonus.\n",
    "\n",
    "Key defaults:\n",
    "- loss_type: BNPO\n",
    "- beta: 0.0 (no separate ref model)\n",
    "- mask_truncated_completions: True\n",
    "- scale_rewards: False (per recent analysis)\n",
    "\n",
    "See the [GRPO Trainer docs](https://huggingface.co/docs/trl/main/en/grpo_trainer) for more configuration tips.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78052536270444b94edc7897bbab7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This may take several minutes depending on steps and GPU\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainer = \u001b[43mtrain_wordle_grpo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecret_word\u001b[49m\u001b[43m=\u001b[49m\u001b[43msecret_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_guesses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_guesses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2.5-7B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# change if you want another compatible chat model\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/anxie/Documents/cookbooks/wordle-grpo-output\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# increase for better results on a capable GPU\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_generations_per_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_prompt_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_completion_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m trainer.save_model()  \u001b[38;5;66;03m# save adapter or full model depending on backend\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wordle/utils.py:401\u001b[39m, in \u001b[36mtrain_wordle_grpo\u001b[39m\u001b[34m(secret_word, past_guesses, model_id, output_dir, max_steps, per_device_train_batch_size, gradient_accumulation_steps, num_generations_per_prompt, max_prompt_length, max_completion_length, beta, learning_rate, seed, generation_batch_size)\u001b[39m\n\u001b[32m    380\u001b[39m config = GRPOConfig(\n\u001b[32m    381\u001b[39m     output_dir=output_dir,\n\u001b[32m    382\u001b[39m     per_device_train_batch_size=per_device_train_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     scale_rewards=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m )\n\u001b[32m    399\u001b[39m reward_fn = make_wordle_reward(secret_word)\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m trainer = \u001b[43mGRPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m trainer.train()\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/trl/trainer/grpo_trainer.py:533\u001b[39m, in \u001b[36mGRPOTrainer.__init__\u001b[39m\u001b[34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[39m\n\u001b[32m    531\u001b[39m     config = AutoConfig.from_pretrained(model_id)\n\u001b[32m    532\u001b[39m     architecture = \u001b[38;5;28mgetattr\u001b[39m(transformers, config.architectures[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     model = \u001b[43marchitecture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_init_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    535\u001b[39m     model_id = model.config._name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/transformers/modeling_utils.py:316\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    318\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/transformers/modeling_utils.py:5061\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5051\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5052\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5054\u001b[39m     (\n\u001b[32m   5055\u001b[39m         model,\n\u001b[32m   5056\u001b[39m         missing_keys,\n\u001b[32m   5057\u001b[39m         unexpected_keys,\n\u001b[32m   5058\u001b[39m         mismatched_keys,\n\u001b[32m   5059\u001b[39m         offload_index,\n\u001b[32m   5060\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5061\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5066\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5067\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5068\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5069\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5070\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5071\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5072\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5074\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5075\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5076\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5077\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5078\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5079\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/transformers/modeling_utils.py:5524\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5521\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5523\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5524\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5525\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5527\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/transformers/modeling_utils.py:974\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/transformers/modeling_utils.py:844\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    842\u001b[39m param = param[...]\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    846\u001b[39m     param = param.contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# This may take several minutes depending on steps and GPU\n",
    "trainer = train_wordle_grpo(\n",
    "    secret_word=secret_word,\n",
    "    past_guesses=past_guesses,\n",
    "    model_id=\"Qwen/Qwen2.5-7B-Instruct\",  # change if you want another compatible chat model\n",
    "    output_dir=\"/Users/anxie/Documents/cookbooks/wordle-grpo-output\",\n",
    "    max_steps=200,  # increase for better results on a capable GPU\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations_per_prompt=8,\n",
    "    max_prompt_length=512,\n",
    "    max_completion_length=64,\n",
    "    beta=0.0,\n",
    "    learning_rate=1e-6,\n",
    "    seed=42,\n",
    "    generation_batch_size=8,\n",
    ")\n",
    "\n",
    "trainer.save_model()  # save adapter or full model depending on backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate after GRPO training\n",
    "from utils import render_prompt, extract_guess, get_feedback, GuessWithFeedback\n",
    "\n",
    "# Use the trainer's active model for generation to see improvements.\n",
    "trained_model = trainer.model\n",
    "trained_tokenizer = trainer.tokenizer if hasattr(trainer, \"tokenizer\") else None\n",
    "\n",
    "# Prepare a fresh prompt (same past_guesses context)\n",
    "prompt = render_prompt(past_guesses)\n",
    "\n",
    "import torch\n",
    "transformers = __import__(\"transformers\")\n",
    "\n",
    "inputs = (trained_tokenizer or __import__(\"transformers\").AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", trust_remote_code=True)).__call__(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(trained_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=(trained_tokenizer or __import__(\"transformers\").AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", trust_remote_code=True)).eos_token_id,\n",
    "    )\n",
    "\n",
    "text = (trained_tokenizer or __import__(\"transformers\").AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", trust_remote_code=True)).decode(\n",
    "    output_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    ")\n",
    "print(text)\n",
    "\n",
    "# Extract and score the guess\n",
    "post_guess = extract_guess(text)\n",
    "print(\"\\nPost-training guess:\", post_guess)\n",
    "print(\"Feedback:\", GuessWithFeedback.from_secret(post_guess, secret_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and reload (optional)\n",
    "\n",
    "Below shows how to push the result to the output directory and reload the model for inference later. Depending on your backend and config, `trainer.save_model()` may save adapters or a full model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload for inference (if needed later)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "reload_dir = \"/Users/anxie/Documents/cookbooks/wordle-grpo-output\"\n",
    "print(\"Reloading from:\", reload_dir)\n",
    "\n",
    "reload_tok = AutoTokenizer.from_pretrained(reload_dir, trust_remote_code=True)\n",
    "reload_model = AutoModelForCausalLM.from_pretrained(reload_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "prompt = render_prompt(past_guesses)\n",
    "inputs = reload_tok(prompt, return_tensors=\"pt\").to(reload_model.device)\n",
    "with torch.no_grad():\n",
    "    out_ids = reload_model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.95)\n",
    "print(reload_tok.decode(out_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
